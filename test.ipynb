{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 13 22:17:00 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 511.23       Driver Version: 511.23       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   47C    P8    13W /  N/A |      0MiB /  6144MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     17380    C+G   ...n64\\EpicGamesLauncher.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!conda install pytorch torchvision torchaudio cudatoolkit=11.0 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: diffusers==0.11.1 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (0.11.1)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from diffusers==0.11.1) (6.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.10.0 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from diffusers==0.11.1) (0.13.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from diffusers==0.11.1) (1.24.2)\n",
      "Requirement already satisfied: requests in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from diffusers==0.11.1) (2.28.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from diffusers==0.11.1) (2022.10.31)\n",
      "Requirement already satisfied: Pillow in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from diffusers==0.11.1) (9.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from diffusers==0.11.1) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from huggingface-hub>=0.10.0->diffusers==0.11.1) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from huggingface-hub>=0.10.0->diffusers==0.11.1) (4.5.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from huggingface-hub>=0.10.0->diffusers==0.11.1) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from huggingface-hub>=0.10.0->diffusers==0.11.1) (6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from importlib-metadata->diffusers==0.11.1) (3.15.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from requests->diffusers==0.11.1) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from requests->diffusers==0.11.1) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from requests->diffusers==0.11.1) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from requests->diffusers==0.11.1) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.10.0->diffusers==0.11.1) (0.4.6)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "     ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.1/6.3 MB 1.1 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 0.8/6.3 MB 6.6 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 2.1/6.3 MB 13.3 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 2.1/6.3 MB 13.3 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 2.2/6.3 MB 8.1 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 4.4/6.3 MB 14.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.3/6.3 MB 18.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 6.3/6.3 MB 16.2 MB/s eta 0:00:00\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.10.1-cp39-cp39-win_amd64.whl (42.5 MB)\n",
      "     ---------------------------------------- 0.0/42.5 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 2.2/42.5 MB 47.5 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 3.6/42.5 MB 38.7 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 5.7/42.5 MB 40.2 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 8.2/42.5 MB 43.5 MB/s eta 0:00:01\n",
      "     --------- ----------------------------- 10.4/42.5 MB 46.7 MB/s eta 0:00:01\n",
      "     ---------- ---------------------------- 12.0/42.5 MB 43.5 MB/s eta 0:00:01\n",
      "     ------------- ------------------------- 14.2/42.5 MB 43.7 MB/s eta 0:00:01\n",
      "     -------------- ------------------------ 15.5/42.5 MB 46.7 MB/s eta 0:00:01\n",
      "     ---------------- ---------------------- 17.4/42.5 MB 40.9 MB/s eta 0:00:01\n",
      "     ----------------- --------------------- 19.1/42.5 MB 40.9 MB/s eta 0:00:01\n",
      "     ------------------ -------------------- 20.6/42.5 MB 38.6 MB/s eta 0:00:01\n",
      "     -------------------- ------------------ 22.5/42.5 MB 40.9 MB/s eta 0:00:01\n",
      "     --------------------- ----------------- 23.9/42.5 MB 34.4 MB/s eta 0:00:01\n",
      "     ----------------------- --------------- 26.0/42.5 MB 36.3 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 28.1/42.5 MB 38.6 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 30.9/42.5 MB 46.9 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 33.0/42.5 MB 46.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 35.0/42.5 MB 46.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 37.4/42.5 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 39.2/42.5 MB 50.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  41.6/42.5 MB 43.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.5/42.5 MB 43.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.5/42.5 MB 43.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.5/42.5 MB 43.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.5/42.5 MB 43.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.5/42.5 MB 43.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 42.5/42.5 MB 21.1 MB/s eta 0:00:00\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "     ---------------------------------------- 0.0/53.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 53.1/53.1 kB ? eta 0:00:00\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.17.1-py3-none-any.whl (212 kB)\n",
      "     ---------------------------------------- 0.0/212.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 212.8/212.8 kB ? eta 0:00:00\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: requests in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from transformers) (6.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 0.0/3.3 MB ? eta -:--:--\n",
      "     -------------------- ------------------- 1.7/3.3 MB 36.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  3.3/3.3 MB 42.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.3/3.3 MB 30.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from ftfy) (0.2.6)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from accelerate) (1.12.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Installing collected packages: tokenizers, scipy, ftfy, accelerate, transformers\n",
      "Successfully installed accelerate-0.17.1 ftfy-6.1.1 scipy-1.10.1 tokenizers-0.13.2 transformers-4.26.1\n"
     ]
    }
   ],
   "source": [
    "!pip install diffusers==0.11.1\n",
    "!pip install transformers scipy ftfy accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bf1d5ac850449490eef88037fe26ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ain/model_index.json:   0%|          | 0.00/545 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kevsu\\.cache\\huggingface\\diffusers. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a824ef011d42128848f8af080341bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40494a0c20284a9db0cd77ecab36ad76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4690b773ded4569ae56ecf8c1a711f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef1495ea9f24f5d9bf7ba88491313c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/460 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c36b3fa56774a9d82484cc76e816637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2854007bc5bc490a8f494b063cbc0976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/807 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d45f3ecf01476399904e0303f5efa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_encoder/config.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0edf4a5dc5c849598dac239076209975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d61b6ef4dbd4fab92a759771bd4653a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cheduler_config.json:   0%|          | 0.00/346 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88583990d9c1439fa63ad944bec42f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53cefc8f00a4ff8ba84ff82cd66fe8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)1bc/unet/config.json:   0%|          | 0.00/911 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3ec4cf5d384784bc862ea813079fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)on_pytorch_model.bin:   0%|          | 0.00/3.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb37df7665a45a4958803ebead3aa47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ch_model.safetensors:   0%|          | 0.00/3.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b09385e8b24df49370669432f4f88c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)on_pytorch_model.bin:   0%|          | 0.00/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5d3a7518cb443ca2b50e6d217efdf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)71bc/vae/config.json:   0%|          | 0.00/553 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a718a807d563483ea647a79658ac4e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages\\transformers\\models\\clip\\feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", torch_dtype=torch.float16)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pipe \u001b[39m=\u001b[39m pipe\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages\\diffusers\\pipeline_utils.py:270\u001b[0m, in \u001b[0;36mDiffusionPipeline.to\u001b[1;34m(self, torch_device)\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[39mif\u001b[39;00m module\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16 \u001b[39mand\u001b[39;00m \u001b[39mstr\u001b[39m(torch_device) \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    263\u001b[0m             logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    264\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mPipelines loaded with `torch_dtype=torch.float16` cannot run with `cpu` device. It\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    265\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is not recommended to move them to `cpu` as running them will fail. Please make\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    268\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m `torch_dtype=torch.float16` argument, or use another device for inference.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    269\u001b[0m             )\n\u001b[1;32m--> 270\u001b[0m         module\u001b[39m.\u001b[39;49mto(torch_device)\n\u001b[0;32m    271\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages\\transformers\\modeling_utils.py:1749\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1744\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1745\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`.to` is not supported for `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1746\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1747\u001b[0m     )\n\u001b[0;32m   1748\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1749\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mto(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    924\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m    925\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> 927\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32mc:\\Users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages\\torch\\nn\\modules\\module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 602\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    603\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    604\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages\\torch\\nn\\modules\\module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m    923\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    924\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> 925\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[1;32mc:\\Users\\kevsu\\anaconda3\\envs\\ai-stuff\\lib\\site-packages\\torch\\cuda\\__init__.py:211\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    210\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    212\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    214\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "pipe = pipe.to(\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-stuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
